{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siZTlWYzkw6o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdWIPB1bkw6p"
      },
      "outputs": [],
      "source": [
        "zip_file = keras.utils.get_file(\n",
        "    fname=\"cora.tgz\",\n",
        "    origin=\"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\",\n",
        "    extract=True,\n",
        ")\n",
        "data_dir = os.path.join(os.path.dirname(zip_file), \"cora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybdJOEGrkw6q"
      },
      "source": [
        "### Process and visualize the dataset\n",
        "\n",
        "Then we load the citations data into a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAsk8ZX-kw6q"
      },
      "outputs": [],
      "source": [
        "citations = pd.read_csv(\n",
        "    os.path.join(data_dir, \"cora.cites\"),\n",
        "    sep=\"\\t\",\n",
        "    header=None,\n",
        "    names=[\"target\", \"source\"],\n",
        ")\n",
        "print(\"Citations shape:\", citations.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyJMYHdQkw6r"
      },
      "outputs": [],
      "source": [
        "citations.sample(frac=1).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thUBGEa-kw6r"
      },
      "source": [
        "Now let's load the papers data into a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5S7hJ3p3kw6s"
      },
      "outputs": [],
      "source": [
        "column_names = [\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"]\n",
        "papers = pd.read_csv(\n",
        "    os.path.join(data_dir, \"cora.content\"), sep=\"\\t\", header=None, names=column_names,\n",
        ")\n",
        "print(\"Papers shape:\", papers.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEjvTWpukw6s"
      },
      "outputs": [],
      "source": [
        "print(papers.sample(5).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAKBkqdXkw6t"
      },
      "source": [
        "Let's display the count of the papers in each subject."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eddCO1JZkw6t"
      },
      "outputs": [],
      "source": [
        "print(papers.subject.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT8BHt36kw6t"
      },
      "source": [
        "We convert the paper ids and the subjects into zero-based indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOgy_vY9kw6t"
      },
      "outputs": [],
      "source": [
        "class_values = sorted(papers[\"subject\"].unique())\n",
        "class_idx = {name: id for id, name in enumerate(class_values)}\n",
        "paper_idx = {name: idx for idx, name in enumerate(sorted(papers[\"paper_id\"].unique()))}\n",
        "\n",
        "papers[\"paper_id\"] = papers[\"paper_id\"].apply(lambda name: paper_idx[name])\n",
        "citations[\"source\"] = citations[\"source\"].apply(lambda name: paper_idx[name])\n",
        "citations[\"target\"] = citations[\"target\"].apply(lambda name: paper_idx[name])\n",
        "papers[\"subject\"] = papers[\"subject\"].apply(lambda value: class_idx[value])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-9xIEPxkw6u"
      },
      "source": [
        "Now let's visualize the citation graph. Each node in the graph represents a paper,\n",
        "and the color of the node corresponds to its subject. Note that we only show a sample of\n",
        "the papers in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmSOZFKikw6u"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "colors = papers[\"subject\"].tolist()\n",
        "cora_graph = nx.from_pandas_edgelist(citations.sample(n=1500))\n",
        "subjects = list(papers[papers[\"paper_id\"].isin(list(cora_graph.nodes))][\"subject\"])\n",
        "nx.draw_spring(cora_graph, node_size=15, node_color=subjects)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-7QHRhykw6u"
      },
      "source": [
        "### Split the dataset into stratified train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sG0hwU7kw6u"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = [], []\n",
        "\n",
        "for _, group_data in papers.groupby(\"subject\"):\n",
        "    # Select around 50% of the dataset for training.\n",
        "    random_selection = np.random.rand(len(group_data.index)) <= 0.5\n",
        "    train_data.append(group_data[random_selection])\n",
        "    test_data.append(group_data[~random_selection])\n",
        "\n",
        "train_data = pd.concat(train_data).sample(frac=1)\n",
        "test_data = pd.concat(test_data).sample(frac=1)\n",
        "\n",
        "print(\"Train data shape:\", train_data.shape)\n",
        "print(\"Test data shape:\", test_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlOK3B-Jkw6v"
      },
      "source": [
        "## Implement Train and Evaluate Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-L3OGN9kw6v"
      },
      "outputs": [],
      "source": [
        "hidden_units = [32, 32]\n",
        "learning_rate = 0.01\n",
        "dropout_rate = 0.5\n",
        "num_epochs = 300\n",
        "batch_size = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XlhBiOAkw6v"
      },
      "source": [
        "This function compiles and trains an input model using the given training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpcrmjuHkw6v"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_experiment(model, x_train, y_train):\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        "    )\n",
        "    # Create an early stopping callback.\n",
        "    early_stopping = keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_acc\", patience=50, restore_best_weights=True\n",
        "    )\n",
        "    # Fit the model.\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        epochs=num_epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.15,\n",
        "        callbacks=[early_stopping],\n",
        "    )\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9msxh58kw6w"
      },
      "source": [
        "This function displays the loss and accuracy curves of the model during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPXlyC5Nkw6w"
      },
      "outputs": [],
      "source": [
        "\n",
        "def display_learning_curves(history):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    ax1.plot(history.history[\"loss\"])\n",
        "    ax1.plot(history.history[\"val_loss\"])\n",
        "    ax1.legend([\"train\", \"test\"], loc=\"upper right\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "\n",
        "    ax2.plot(history.history[\"acc\"])\n",
        "    ax2.plot(history.history[\"val_acc\"])\n",
        "    ax2.legend([\"train\", \"test\"], loc=\"upper right\")\n",
        "    ax2.set_xlabel(\"Epochs\")\n",
        "    ax2.set_ylabel(\"Accuracy\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pckFMNdAkw6w"
      },
      "source": [
        "## Implement Feedforward Network (FFN) Module\n",
        "\n",
        "We will use this module in the baseline and the GNN models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL4RtE6Tkw6w"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_ffn(hidden_units, dropout_rate, name=None):\n",
        "    fnn_layers = []\n",
        "\n",
        "    for units in hidden_units:\n",
        "        fnn_layers.append(layers.BatchNormalization())\n",
        "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
        "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
        "\n",
        "    return keras.Sequential(fnn_layers, name=name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI2Y8miVkw6w"
      },
      "source": [
        "## Build a Baseline Neural Network Model\n",
        "\n",
        "### Prepare the data for the baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j183921kw6x"
      },
      "outputs": [],
      "source": [
        "feature_names = set(papers.columns) - {\"paper_id\", \"subject\"}\n",
        "num_features = len(feature_names)\n",
        "num_classes = len(class_idx)\n",
        "\n",
        "# Create train and test features as a numpy array.\n",
        "x_train = train_data[feature_names].to_numpy()\n",
        "x_test = test_data[feature_names].to_numpy()\n",
        "# Create train and test targets as a numpy array.\n",
        "y_train = train_data[\"subject\"]\n",
        "y_test = test_data[\"subject\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyJA4gp8kw6x"
      },
      "source": [
        "### Implement a baseline classifier\n",
        "\n",
        "We add five FFN blocks with skip connections, so that we generate a baseline model with\n",
        "roughly the same number of parameters as the GNN models to be built later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noGMhhyYkw6x"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_baseline_model(hidden_units, num_classes, dropout_rate=0.2):\n",
        "    inputs = layers.Input(shape=(num_features,), name=\"input_features\")\n",
        "    x = create_ffn(hidden_units, dropout_rate, name=f\"ffn_block1\")(inputs)\n",
        "    for block_idx in range(4):\n",
        "        # Create an FFN block.\n",
        "        x1 = create_ffn(hidden_units, dropout_rate, name=f\"ffn_block{block_idx + 2}\")(x)\n",
        "        # Add skip connection.\n",
        "        x = layers.Add(name=f\"skip_connection{block_idx + 2}\")([x, x1])\n",
        "    # Compute logits.\n",
        "    logits = layers.Dense(num_classes, name=\"logits\")(x)\n",
        "    # Create the model.\n",
        "    return keras.Model(inputs=inputs, outputs=logits, name=\"baseline\")\n",
        "\n",
        "\n",
        "baseline_model = create_baseline_model(hidden_units, num_classes, dropout_rate)\n",
        "baseline_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGe010iekw6x"
      },
      "source": [
        "### Train the baseline classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6KUrLDukw6x"
      },
      "outputs": [],
      "source": [
        "history = run_experiment(baseline_model, x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fjy_Ypzkw6y"
      },
      "source": [
        "Let's plot the learning curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0CGREkckw6y"
      },
      "outputs": [],
      "source": [
        "display_learning_curves(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu3CrAxykw6y"
      },
      "source": [
        "Now we evaluate the baseline model on the test data split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzptnTuBkw6y"
      },
      "outputs": [],
      "source": [
        "_, test_accuracy = baseline_model.evaluate(x=x_test, y=y_test, verbose=0)\n",
        "print(f\"Test accuracy: {round(test_accuracy * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROilSX1Zkw6y"
      },
      "source": [
        "### Examine the baseline model predictions\n",
        "\n",
        "Let's create new data instances by randomly generating binary word vectors with respect to\n",
        "the word presence probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NLp7K7ukw6z"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_random_instances(num_instances):\n",
        "    token_probability = x_train.mean(axis=0)\n",
        "    instances = []\n",
        "    for _ in range(num_instances):\n",
        "        probabilities = np.random.uniform(size=len(token_probability))\n",
        "        instance = (probabilities <= token_probability).astype(int)\n",
        "        instances.append(instance)\n",
        "\n",
        "    return np.array(instances)\n",
        "\n",
        "\n",
        "def display_class_probabilities(probabilities):\n",
        "    for instance_idx, probs in enumerate(probabilities):\n",
        "        print(f\"Instance {instance_idx + 1}:\")\n",
        "        for class_idx, prob in enumerate(probs):\n",
        "            print(f\"- {class_values[class_idx]}: {round(prob * 100, 2)}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_4Acwtgkw6z"
      },
      "source": [
        "Now we show the baseline model predictions given these randomly generated instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utkT3lcqkw6z"
      },
      "outputs": [],
      "source": [
        "new_instances = generate_random_instances(num_classes)\n",
        "logits = baseline_model.predict(new_instances)\n",
        "probabilities = keras.activations.softmax(tf.convert_to_tensor(logits)).numpy()\n",
        "display_class_probabilities(probabilities)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ueks5Ec2kw6z"
      },
      "source": [
        "## Build a Graph Neural Network Model\n",
        "\n",
        "### Prepare the data for the graph model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4NDfRE6kw6z"
      },
      "outputs": [],
      "source": [
        "# Create an edges array (sparse adjacency matrix) of shape [2, num_edges].\n",
        "edges = citations[[\"source\", \"target\"]].to_numpy().T\n",
        "# Create an edge weights array of ones.\n",
        "edge_weights = tf.ones(shape=edges.shape[1])\n",
        "# Create a node features array of shape [num_nodes, num_features].\n",
        "node_features = tf.cast(\n",
        "    papers.sort_values(\"paper_id\")[feature_names].to_numpy(), dtype=tf.dtypes.float32\n",
        ")\n",
        "# Create graph info tuple with node_features, edges, and edge_weights.\n",
        "graph_info = (node_features, edges, edge_weights)\n",
        "\n",
        "print(\"Edges shape:\", edges.shape)\n",
        "print(\"Nodes shape:\", node_features.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AfhiRIKqkw60"
      },
      "source": [
        "### Implement a graph convolution layer\n",
        "\n",
        "\n",
        "\n",
        "The technique implemented use ideas from [Graph Convolutional Networks](https://arxiv.org/abs/1609.02907),\n",
        "[GraphSage](https://arxiv.org/abs/1706.02216), [Graph Isomorphism Network](https://arxiv.org/abs/1810.00826),\n",
        "[Simple Graph Networks](https://arxiv.org/abs/1902.07153), and\n",
        "[Gated Graph Sequence Neural Networks](https://arxiv.org/abs/1511.05493).\n",
        "Two other key techniques that are not covered are [Graph Attention Networks](https://arxiv.org/abs/1710.10903)\n",
        "and [Message Passing Neural Networks](https://arxiv.org/abs/1704.01212)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f9Mh4nikw60"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GraphConvLayer(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_units,\n",
        "        dropout_rate=0.2,\n",
        "        aggregation_type=\"mean\",\n",
        "        combination_type=\"concat\",\n",
        "        normalize=False,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.aggregation_type = aggregation_type\n",
        "        self.combination_type = combination_type\n",
        "        self.normalize = normalize\n",
        "\n",
        "        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)\n",
        "        if self.combination_type == \"gated\":\n",
        "            self.update_fn = layers.GRU(\n",
        "                units=hidden_units,\n",
        "                activation=\"tanh\",\n",
        "                recurrent_activation=\"sigmoid\",\n",
        "                dropout=dropout_rate,\n",
        "                return_state=True,\n",
        "                recurrent_dropout=dropout_rate,\n",
        "            )\n",
        "        else:\n",
        "            self.update_fn = create_ffn(hidden_units, dropout_rate)\n",
        "\n",
        "    def prepare(self, node_repesentations, weights=None):\n",
        "        # node_repesentations shape is [num_edges, embedding_dim].\n",
        "        messages = self.ffn_prepare(node_repesentations)\n",
        "        if weights is not None:\n",
        "            messages = messages * tf.expand_dims(weights, -1)\n",
        "        return messages\n",
        "\n",
        "    def aggregate(self, node_indices, neighbour_messages, node_repesentations):\n",
        "        # node_indices shape is [num_edges].\n",
        "        # neighbour_messages shape: [num_edges, representation_dim].\n",
        "        # node_repesentations shape is [num_nodes, representation_dim].\n",
        "        num_nodes = node_repesentations.shape[0]\n",
        "        if self.aggregation_type == \"sum\":\n",
        "            aggregated_message = tf.math.unsorted_segment_sum(\n",
        "                neighbour_messages, node_indices, num_segments=num_nodes\n",
        "            )\n",
        "        elif self.aggregation_type == \"mean\":\n",
        "            aggregated_message = tf.math.unsorted_segment_mean(\n",
        "                neighbour_messages, node_indices, num_segments=num_nodes\n",
        "            )\n",
        "        elif self.aggregation_type == \"max\":\n",
        "            aggregated_message = tf.math.unsorted_segment_max(\n",
        "                neighbour_messages, node_indices, num_segments=num_nodes\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}.\")\n",
        "\n",
        "        return aggregated_message\n",
        "\n",
        "    def update(self, node_repesentations, aggregated_messages):\n",
        "        # node_repesentations shape is [num_nodes, representation_dim].\n",
        "        # aggregated_messages shape is [num_nodes, representation_dim].\n",
        "        if self.combination_type == \"gru\":\n",
        "            # Create a sequence of two elements for the GRU layer.\n",
        "            h = tf.stack([node_repesentations, aggregated_messages], axis=1)\n",
        "        elif self.combination_type == \"concat\":\n",
        "            # Concatenate the node_repesentations and aggregated_messages.\n",
        "            h = tf.concat([node_repesentations, aggregated_messages], axis=1)\n",
        "        elif self.combination_type == \"add\":\n",
        "            # Add node_repesentations and aggregated_messages.\n",
        "            h = node_repesentations + aggregated_messages\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
        "\n",
        "        # Apply the processing function.\n",
        "        node_embeddings = self.update_fn(h)\n",
        "        if self.combination_type == \"gru\":\n",
        "            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]\n",
        "\n",
        "        if self.normalize:\n",
        "            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)\n",
        "        return node_embeddings\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Process the inputs to produce the node_embeddings.\n",
        "\n",
        "        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.\n",
        "        Returns: node_embeddings of shape [num_nodes, representation_dim].\n",
        "        \"\"\"\n",
        "\n",
        "        node_repesentations, edges, edge_weights = inputs\n",
        "        # Get node_indices (source) and neighbour_indices (target) from edges.\n",
        "        node_indices, neighbour_indices = edges[0], edges[1]\n",
        "        # neighbour_repesentations shape is [num_edges, representation_dim].\n",
        "        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n",
        "\n",
        "        # Prepare the messages of the neighbours.\n",
        "        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)\n",
        "        # Aggregate the neighbour messages.\n",
        "        aggregated_messages = self.aggregate(\n",
        "            node_indices, neighbour_messages, node_repesentations\n",
        "        )\n",
        "        # Update the node embedding with the neighbour messages.\n",
        "        return self.update(node_repesentations, aggregated_messages)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V_uUc0p_kw60"
      },
      "source": [
        "### Implement a graph neural network node classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXkPJhTykw61"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GNNNodeClassifier(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        graph_info,\n",
        "        num_classes,\n",
        "        hidden_units,\n",
        "        aggregation_type=\"sum\",\n",
        "        combination_type=\"concat\",\n",
        "        dropout_rate=0.2,\n",
        "        normalize=True,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
        "        node_features, edges, edge_weights = graph_info\n",
        "        self.node_features = node_features\n",
        "        self.edges = edges\n",
        "        self.edge_weights = edge_weights\n",
        "        # Set edge_weights to ones if not provided.\n",
        "        if self.edge_weights is None:\n",
        "            self.edge_weights = tf.ones(shape=edges.shape[1])\n",
        "        # Scale edge_weights to sum to 1.\n",
        "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
        "\n",
        "        # Create a process layer.\n",
        "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
        "        # Create the first GraphConv layer.\n",
        "        self.conv1 = GraphConvLayer(\n",
        "            hidden_units,\n",
        "            dropout_rate,\n",
        "            aggregation_type,\n",
        "            combination_type,\n",
        "            normalize,\n",
        "            name=\"graph_conv1\",\n",
        "        )\n",
        "        # Create the second GraphConv layer.\n",
        "        self.conv2 = GraphConvLayer(\n",
        "            hidden_units,\n",
        "            dropout_rate,\n",
        "            aggregation_type,\n",
        "            combination_type,\n",
        "            normalize,\n",
        "            name=\"graph_conv2\",\n",
        "        )\n",
        "        # Create a postprocess layer.\n",
        "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
        "        # Create a compute logits layer.\n",
        "        self.compute_logits = layers.Dense(units=num_classes, name=\"logits\")\n",
        "\n",
        "    def call(self, input_node_indices):\n",
        "        # Preprocess the node_features to produce node representations.\n",
        "        x = self.preprocess(self.node_features)\n",
        "        # Apply the first graph conv layer.\n",
        "        x1 = self.conv1((x, self.edges, self.edge_weights))\n",
        "        # Skip connection.\n",
        "        x = x1 + x\n",
        "        # Apply the second graph conv layer.\n",
        "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
        "        # Skip connection.\n",
        "        x = x2 + x\n",
        "        # Postprocess node embedding.\n",
        "        x = self.postprocess(x)\n",
        "        # Fetch node embeddings for the input node_indices.\n",
        "        node_embeddings = tf.gather(x, input_node_indices)\n",
        "        # Compute logits\n",
        "        return self.compute_logits(node_embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjnTOsDIkw61"
      },
      "source": [
        "Let's test instantiating and calling the GNN model.\n",
        "Notice that if you provide `N` node indices, the output will be a tensor of shape `[N, num_classes]`,\n",
        "regardless of the size of the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCR9IDHUkw62"
      },
      "outputs": [],
      "source": [
        "gnn_model = GNNNodeClassifier(\n",
        "    graph_info=graph_info,\n",
        "    num_classes=num_classes,\n",
        "    hidden_units=hidden_units,\n",
        "    dropout_rate=dropout_rate,\n",
        "    name=\"gnn_model\",\n",
        ")\n",
        "\n",
        "print(\"GNN output shape:\", gnn_model([1, 10, 100]))\n",
        "\n",
        "gnn_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4fMTu1Xkw62"
      },
      "source": [
        "### Train the GNN model\n",
        "\n",
        "Note that we use the standard *supervised* cross-entropy loss to train the model.\n",
        "However, we can add another *self-supervised* loss term for the generated node embeddings\n",
        "that makes sure that neighbouring nodes in graph have similar representations, while faraway\n",
        "nodes have dissimilar representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6Km71GPkw62"
      },
      "outputs": [],
      "source": [
        "x_train = train_data.paper_id.to_numpy()\n",
        "history = run_experiment(gnn_model, x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqbQPGLkkw63"
      },
      "source": [
        "Let's plot the learning curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUdq-0B0kw63"
      },
      "outputs": [],
      "source": [
        "display_learning_curves(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9cGIQ_Fkw63"
      },
      "source": [
        "Now we evaluate the GNN model on the test data split.\n",
        "The results may vary depending on the training sample, however the GNN model always outperforms\n",
        "the baseline model in terms of the test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET3EUz9Pkw63"
      },
      "outputs": [],
      "source": [
        "x_test = test_data.paper_id.to_numpy()\n",
        "_, test_accuracy = gnn_model.evaluate(x=x_test, y=y_test, verbose=0)\n",
        "print(f\"Test accuracy: {round(test_accuracy * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2Akh4BCkw63"
      },
      "source": [
        "### Examine the GNN model predictions\n",
        "\n",
        "Let's add the new instances as nodes to the `node_features`, and generate links\n",
        "(citations) to existing nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "840vuW-3kw63"
      },
      "outputs": [],
      "source": [
        "# First we add the N new_instances as nodes to the graph\n",
        "# by appending the new_instance to node_features.\n",
        "num_nodes = node_features.shape[0]\n",
        "new_node_features = np.concatenate([node_features, new_instances])\n",
        "# Second we add the M edges (citations) from each new node to a set\n",
        "# of existing nodes in a particular subject\n",
        "new_node_indices = [i + num_nodes for i in range(num_classes)]\n",
        "new_citations = []\n",
        "for subject_idx, group in papers.groupby(\"subject\"):\n",
        "    subject_papers = list(group.paper_id)\n",
        "    # Select random x papers specific subject.\n",
        "    selected_paper_indices1 = np.random.choice(subject_papers, 5)\n",
        "    # Select random y papers from any subject (where y < x).\n",
        "    selected_paper_indices2 = np.random.choice(list(papers.paper_id), 2)\n",
        "    # Merge the selected paper indices.\n",
        "    selected_paper_indices = np.concatenate(\n",
        "        [selected_paper_indices1, selected_paper_indices2], axis=0\n",
        "    )\n",
        "    # Create edges between a citing paper idx and the selected cited papers.\n",
        "    citing_paper_indx = new_node_indices[subject_idx]\n",
        "    for cited_paper_idx in selected_paper_indices:\n",
        "        new_citations.append([citing_paper_indx, cited_paper_idx])\n",
        "\n",
        "new_citations = np.array(new_citations).T\n",
        "new_edges = np.concatenate([edges, new_citations], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEYbqUI6kw64"
      },
      "source": [
        "Now let's update the `node_features` and the `edges` in the GNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDb4pca4kw64"
      },
      "outputs": [],
      "source": [
        "print(\"Original node_features shape:\", gnn_model.node_features.shape)\n",
        "print(\"Original edges shape:\", gnn_model.edges.shape)\n",
        "gnn_model.node_features = new_node_features\n",
        "gnn_model.edges = new_edges\n",
        "gnn_model.edge_weights = tf.ones(shape=new_edges.shape[1])\n",
        "print(\"New node_features shape:\", gnn_model.node_features.shape)\n",
        "print(\"New edges shape:\", gnn_model.edges.shape)\n",
        "\n",
        "logits = gnn_model.predict(tf.convert_to_tensor(new_node_indices))\n",
        "probabilities = keras.activations.softmax(tf.convert_to_tensor(logits)).numpy()\n",
        "display_class_probabilities(probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRK_lUVHkw64"
      },
      "source": [
        "Notice that the probabilities of the expected subjects\n",
        "(to which several citations are added) are higher compared to the baseline model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gnn_citations",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
